{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtakimoto/flight_delay/blob/main/tree/main/api/MachineLearning/notebooks/SmallFlightDelay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lejEmLi-9ukZ"
      },
      "source": [
        "#Geracao de Modelo para analise de atraso de voo\n",
        "Instalando as bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8jcTI75BELH"
      },
      "source": [
        "Bibliotecas necess√°rias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8KDsISE9tgX"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# configura√ß√£o para n√£o exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports necess√°rios\n",
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJ5u2t0-FXZ"
      },
      "source": [
        "#Carregamento do dataset (carga de um dataset pequeno para o MVP da PUC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Pn1RTi_oUq",
        "outputId": "82674b97-41a3-42a7-ec6a-66a4ac7d7eab"
      },
      "outputs": [],
      "source": [
        "# Informa a URL de importa√ß√£o do dataset\n",
        "DatasetPath = kagglehub.dataset_download(\"eminhashimi/flight-delay-dataset\")\n",
        "print(\"Path to dataset files:\", DatasetPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "254M-cOsHKiQ",
        "outputId": "866a990b-da00-4ddd-bc89-68c5c1c515e6"
      },
      "outputs": [],
      "source": [
        "FlightDataset = pd.read_csv(DatasetPath+\"/flights_test.csv\", delimiter=',')\n",
        "\n",
        "# displaying the contents of the Test XLSX file\n",
        "FlightDataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnWNMp3ISFD"
      },
      "source": [
        "##Pre Processamento do dataset carregado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvcSNodBIZXE",
        "outputId": "28485ad0-fc2c-45fc-9838-c4175f907f71"
      },
      "outputs": [],
      "source": [
        "FlightDataset.info()\n",
        "\n",
        "# Remove rows with null values\n",
        "FlightDatasetCleaned = FlightDataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBDVTZ1Rrphp",
        "outputId": "d45a76a5-5f11-499b-97d6-508a006a85b1"
      },
      "outputs": [],
      "source": [
        "print('Shape of the resulting Flight Dataset table: ', FlightDataset.shape)\n",
        "print('Shape of the resulting Flight Dataset table: ', FlightDatasetCleaned.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "kvac52uJxY_J",
        "outputId": "eacae939-4fb5-4aba-c5cc-c85c31c932a7"
      },
      "outputs": [],
      "source": [
        "FlightDatasetCleaned['DELAY_DETECTED'] = 0\n",
        "FlightDatasetCleaned.loc[(FlightDatasetCleaned['ARRIVAL_TIME']-FlightDatasetCleaned['SCHEDULED_ARRIVAL']) >0, 'DELAY_DETECTED'] = 1\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po8-isvXRe7l"
      },
      "source": [
        "##Diminui√ß√£o do n√∫mero de registros para o MVP da PUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "y1fWcTM3SZ8n",
        "outputId": "ae60a056-9a8f-47e8-9691-07b36628f9a2"
      },
      "outputs": [],
      "source": [
        "FlightDatasetCleaned = FlightDatasetCleaned.sample(frac=0.02)\n",
        "print('Shape of the resulting Flight Dataset table: ', FlightDatasetCleaned.shape)\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AhgE12ohxdUK",
        "outputId": "4090d976-5749-4c05-a257-f22cd0ec9990"
      },
      "outputs": [],
      "source": [
        "airline_names = FlightDatasetCleaned['AIRLINE'].unique()\n",
        "tail_names = FlightDatasetCleaned['TAIL_NUMBER'].unique()\n",
        "origin_names = FlightDatasetCleaned['ORIGIN_AIRPORT'].unique()\n",
        "destination_names = FlightDatasetCleaned['DESTINATION_AIRPORT'].unique()\n",
        "\n",
        "str_cols = FlightDatasetCleaned.columns[FlightDatasetCleaned.columns.str.contains('(?:AIRLINE|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT)')]\n",
        "clfs = {c:LabelEncoder() for c in str_cols}\n",
        "\n",
        "for col, clf in clfs.items():\n",
        "  FlightDatasetCleaned[col] = clfs[col].fit_transform(FlightDatasetCleaned[col])\n",
        "\n",
        "airline_values = FlightDatasetCleaned['AIRLINE'].unique()\n",
        "tail_values = FlightDatasetCleaned['TAIL_NUMBER'].unique()\n",
        "origin_values = FlightDatasetCleaned['ORIGIN_AIRPORT'].unique()\n",
        "destination_values = FlightDatasetCleaned['DESTINATION_AIRPORT'].unique()\n",
        "\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXPThYRfzwJu",
        "outputId": "d0ecef23-dc10-4332-c11e-cfa5e6a42265"
      },
      "outputs": [],
      "source": [
        "map_airline = np.array([airline_names, airline_values])\n",
        "column_names = ['AIRLINE', 'VALUES']\n",
        "airline_dataframe = pd.DataFrame(map_airline.transpose(), columns=column_names)\n",
        "print(airline_dataframe)\n",
        "\n",
        "map_tail = np.array([tail_names, tail_values])\n",
        "column_names = ['TAIL_NUMBER', 'VALUES']\n",
        "tail_dataframe = pd.DataFrame(map_tail.transpose(), columns=column_names)\n",
        "print(tail_dataframe)\n",
        "\n",
        "map_origin = np.array([origin_names, origin_values])\n",
        "column_names = ['ORIGIN_AIRPORT', 'VALUES']\n",
        "origin_dataframe = pd.DataFrame(map_origin.transpose(), columns=column_names)\n",
        "print(origin_dataframe)\n",
        "\n",
        "map_destination = np.array([destination_names, destination_values])\n",
        "column_names = ['DESTINATION_AIRPORT', 'VALUES']\n",
        "destination_dataframe = pd.DataFrame(map_destination.transpose(), columns=column_names)\n",
        "print(destination_dataframe)\n",
        "\n",
        "# Define o path\n",
        "directory = 'data'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "airline_dataframe.to_csv(os.path.join(directory, 'airline.csv'), index=False)\n",
        "tail_dataframe.to_csv(os.path.join(directory, 'tail.csv'), index=False)\n",
        "origin_dataframe.to_csv(os.path.join(directory, 'origin.csv'), index=False)\n",
        "destination_dataframe.to_csv(os.path.join(directory, 'destination.csv'), index=False)\n",
        "\n",
        "\n",
        "#print(clfs['AIRLINE'].inverse_transform(FlightDatasetCleaned['AIRLINE']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqtjrGSroA-"
      },
      "source": [
        "# Separa√ß√£o em conjunto de treino e conjunto de teste com holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgyAlfODsCgx"
      },
      "outputs": [],
      "source": [
        "test_size = 0.20 # tamanho do conjunto de teste\n",
        "seed = 7 # semente aleat√≥ria\n",
        "\n",
        "# Separa√ß√£o em conjuntos de treino e teste\n",
        "array = FlightDatasetCleaned.values\n",
        "X = FlightDatasetCleaned[['DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_DELAY', 'SCHEDULED_ARRIVAL']].values\n",
        "y = FlightDatasetCleaned['DELAY_DETECTED'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratifica√ß√£o\n",
        "\n",
        "# Par√¢metros e parti√ß√µes da valida√ß√£o cruzada\n",
        "scoring = 'accuracy'\n",
        "num_particoes = 10\n",
        "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # valida√ß√£o cruzada com estratifica√ß√£o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEHHNyDuNKN"
      },
      "source": [
        "#Modelagem e Infer√™ncia\n",
        "Cria√ß√£o e avalia√ß√£o de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6cNGMPDIuScu",
        "outputId": "dc782e3d-22ec-4b2f-ac4b-500c37408336"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Lista que armazenar√° os modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os par√¢metros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(bases)))\n",
        "\n",
        "# Listas para armazenar os resultados\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Avalia√ß√£o dos modelos (treinamento)\n",
        "for name, model in models:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de compara√ß√£o dos modelos\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "fig.suptitle('Compara√ß√£o dos Modelos')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTQwB_6U95g"
      },
      "source": [
        "# Cria√ß√£o e avalia√ß√£o de modelos: dados padronizados e normalizados\n",
        "\n",
        "Trabalhando com dados pontencialmente desbalanceados ou sens√≠veis a escala\n",
        "\n",
        "StandardScaler (padroniza√ß√£o do conjunto de dados) e MinMaxScaler (normaliza√ß√£o do conjunto de dados) s√£o duas t√©cnicas de normaliza√ß√£o/escala usadas em machine learning para pr√©-processamento de dados e s√£o √∫teis para preparar dados para algoritmos de aprendizado de m√°quina que s√£o sens√≠veis √† escala dos dados.\n",
        "\n",
        "## StandardScaler\n",
        "StandardScaler padroniza os dados, ou seja, remove a m√©dia e escala os dados para que tenham uma vari√¢ncia unit√°ria. Ele transforma os dados para que a m√©dia de cada feature seja 0 e a vari√¢ncia seja 1.\n",
        "\n",
        "F√≥rmula: $z_i=\\frac{x_i-\\mu}{\\sigma}$\n",
        "\n",
        "\n",
        "onde:\n",
        "- $x_i$ √© o valor original do $i$-√©simo termo da feature.\n",
        "- $\\mu$ √© a m√©dia dos valores da feature.\n",
        "- $\\sigma$ √© o desvio padr√£o dos valores da feature.\n",
        "ùë•\n",
        "x √© o valor original da feature.\n",
        "ùúá\n",
        "Œº √© a m√©dia dos valores da feature.\n",
        "ùúé\n",
        "œÉ √© o desvio padr√£o dos valores da feature.\n",
        "\n",
        "\n",
        "## MinMaxScaler\n",
        "MinMaxScaler escala e transforma os dados para um intervalo espec√≠fico, geralmente entre 0 e 1. Ele transforma os dados para que o menor valor de uma feature seja 0 e o maior valor seja 1.\n",
        "\n",
        "F√≥rmula: $z_i=\\frac{x_i-min(x)}{max(x)-min(x)}$\n",
        "\n",
        "onde:\n",
        "- $x_i$ √© o valor original do $i$-√©simo termo da feature.\n",
        "- $min(x)$ √© o menor valor da feature.\n",
        "- $max(x)$ √© o maior valor da feature.\n",
        "\n",
        "N√≥s vamos aplicar essas t√©cnicas para os dados do dataset de diabetes atrav√©s da constru√ß√£o de pipelines. Pipelines s√£o uma maneira de simplificar o processo de constru√ß√£o de modelos, permitindo que voc√™ execute v√°rias etapas de pr√©-processamento e modelagem em sequ√™ncia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uaRFYSSIU_9D",
        "outputId": "db2bb473-827e-412f-b506-6b62cf1c4d4c"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "# Listas para armazenar os armazenar os pipelines e os resultados para todas as vis√µes do dataset\n",
        "pipelines = []\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "\n",
        "# Criando os elementos do pipeline\n",
        "\n",
        "# Algoritmos que ser√£o utilizados\n",
        "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "cart = ('CART', DecisionTreeClassifier())\n",
        "naive_bayes = ('NB', GaussianNB())\n",
        "svm = ('SVM', SVC())\n",
        "bagging = ('Bag', BaggingClassifier(estimator=base, n_estimators=num_trees))\n",
        "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
        "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
        "voting = ('Voting', VotingClassifier(bases))\n",
        "\n",
        "# Transforma√ß√µes que ser√£o utilizadas\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "\n",
        "# Montando os pipelines\n",
        "# A ordem de execu√ß√£o √© da esquerda para a direita.\n",
        "\n",
        "# Dataset original\n",
        "pipelines.append(('LR-orig', Pipeline([reg_log])))\n",
        "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
        "pipelines.append(('CART-orig', Pipeline([cart])))\n",
        "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
        "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
        "pipelines.append(('Bag-orig', Pipeline([bagging])))\n",
        "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
        "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
        "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
        "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
        "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
        "\n",
        "# Dataset Padronizado\n",
        "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log])))\n",
        "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
        "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
        "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
        "pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging])))\n",
        "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
        "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
        "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
        "\n",
        "# Dataset Normalizado\n",
        "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log])))\n",
        "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
        "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
        "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
        "pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging])))\n",
        "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
        "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
        "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
        "\n",
        "# Executando os pipelines\n",
        "for name, model in pipelines:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de compara√ß√£o dos modelos\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "fig.suptitle('Compara√ß√£o dos Modelos - Dataset original, padronizado e normalizado')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names, rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usd_XLdAVjvv"
      },
      "source": [
        "# Otimiza√ß√£o dos hiperpar√¢metros\n",
        "\n",
        "A otimiza√ß√£o de hiperpar√¢metros √© o processo de encontrar os valores ideais para os hiperpar√¢metros de um modelo de machine learning. O objetivo √© encontrar a combina√ß√£o de hiperpar√¢metros que resulta no melhor desempenho do modelo.\n",
        "\n",
        "\n",
        "## Grid Search (*for√ßa bruta*)\n",
        "\n",
        "Como Funciona o Grid Search?\n",
        "1. Defini√ß√£o do Espa√ßo de Hiperpar√¢metros: Primeiro, define-se um conjunto de valores poss√≠veis para cada hiperpar√¢metro.\n",
        "2. Avalia√ß√£o das Combina√ß√µes: Em seguida, cada combina√ß√£o poss√≠vel desses valores √© avaliada.\n",
        "3. Sele√ß√£o do Melhor Conjunto: A combina√ß√£o de hiperpar√¢metros que produz o melhor desempenho √© selecionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FEZ2skqWcqw",
        "outputId": "12899f36-be89-46e6-e576-ae52d97bdcb7"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)  # Definindo uma semente global para este bloco\n",
        "\n",
        "# Lista de modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os par√¢metros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(estimators=bases, voting='hard')))\n",
        "\n",
        "# Definindo os componentes do pipeline\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "# Lista de pipelines\n",
        "pipelines = []\n",
        "\n",
        "# Criando pipelines para cada modelo\n",
        "for name, model in models:\n",
        "    pipelines.append((name + '-orig', Pipeline(steps=[(name, model)])))\n",
        "    pipelines.append((name + '-padr', Pipeline(steps=[standard_scaler, (name, model)])))\n",
        "    pipelines.append((name + '-norm', Pipeline(steps=[min_max_scaler, (name, model)])))\n",
        "\n",
        "# Definindo os par√¢metros para GridSearchCV\n",
        "param_grids = {\n",
        "    'LR': {\n",
        "        'LR__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'LR__solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'KNN': {\n",
        "        'KNN__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
        "        'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
        "    },\n",
        "    'CART': {\n",
        "        'CART__max_depth': [None, 10, 20, 30, 40, 50],\n",
        "        'CART__min_samples_split': [2, 5, 10],\n",
        "        'CART__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'NB': {\n",
        "        'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
        "    },\n",
        "    'SVM': {\n",
        "        #'SVM__C': [0.1, 1, 10, 100],\n",
        "        'SVM__C': [1, 10],\n",
        "        #'SVM__gamma': [1, 0.1, 0.01, 0.001],\n",
        "        'SVM__gamma': [1, 0.1],\n",
        "        'SVM__kernel': ['rbf', 'linear']\n",
        "    },\n",
        "    'RF': {\n",
        "        'RF__n_estimators': [10, 50, 100, 200],\n",
        "        'RF__max_features': ['auto', 'sqrt', 'log2'],\n",
        "        'RF__max_depth': [None, 10, 20, 30],\n",
        "        'RF__min_samples_split': [2, 5, 10],\n",
        "        'RF__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'ET': {\n",
        "        'ET__n_estimators': [10, 50, 100, 200],\n",
        "        'ET__max_features': ['auto', 'sqrt', 'log2'],\n",
        "        'ET__max_depth': [None, 10, 20, 30],\n",
        "        'ET__min_samples_split': [2, 5, 10],\n",
        "        'ET__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Ada': {\n",
        "        'Ada__n_estimators': [10, 50, 100, 200],\n",
        "        'Ada__learning_rate': [0.01, 0.1, 1, 10]\n",
        "    },\n",
        "    'GB': {\n",
        "        'GB__n_estimators': [10, 50, 100, 200],\n",
        "        'GB__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "        'GB__max_depth': [3, 5, 7, 9]\n",
        "    },\n",
        "    'Voting': {\n",
        "        # Para VotingClassifier, geralmente n√£o h√° hiperpar√¢metros para ajustar diretamente\n",
        "        # Ajustar os hiperpar√¢metros dos estimadores base individualmente se necess√°rio\n",
        "    }\n",
        "}\n",
        "\n",
        "# Par√¢metros de cross-validation e scoring\n",
        "scoring = 'accuracy'\n",
        "kfold = 5\n",
        "\n",
        "# Executando o GridSearchCV para cada pipeline\n",
        "for name, pipeline in pipelines:\n",
        "    model_type = name.split('-')[0]\n",
        "    if model_type in param_grids:\n",
        "        param_grid = param_grids[model_type]\n",
        "    else:\n",
        "        param_grid = {}  # Para modelos que n√£o t√™m par√¢metros definidos\n",
        "\n",
        "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "    # Imprimindo a melhor configura√ß√£o\n",
        "    print(\"Modelo: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-usY_T1yWvMN",
        "outputId": "7dff6ca1-0a90-42ca-d716-1eaddd9d9af1"
      },
      "outputs": [],
      "source": [
        "# Tuning do KNN\n",
        "\n",
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "pipelines = []\n",
        "\n",
        "# Definindo os componentes do pipeline\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "pipelines.append(('knn-orig', Pipeline(steps=[knn])))\n",
        "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
        "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
        "\n",
        "param_grid = {\n",
        "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
        "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
        "}\n",
        "\n",
        "# Prepara e executa o GridSearchCV\n",
        "for name, model in pipelines:\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "    # imprime a melhor configura√ß√£o\n",
        "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcdhA7WnWzIo"
      },
      "source": [
        "# Finaliza√ß√£o do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyI-RG0lW2PU",
        "outputId": "e7b75644-a644-46b5-e7a2-2224f9fb1d38"
      },
      "outputs": [],
      "source": [
        "# Avalia√ß√£o do modelo com o conjunto de testes\n",
        "# Melhor modelo\n",
        "# Modelo: GB-norm - Melhor: 0.823716 usando {'GB__learning_rate': 0.1, 'GB__max_depth': 3, 'GB__n_estimators': 50}\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "# Prepara√ß√£o do modelo\n",
        "scaler = MinMaxScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train) # aplica√ß√£o da normaliza√ß√£o no conjunto de treino\n",
        "#model = RandomForestClassifier(n_estimators=100,\n",
        "#                               max_features='sqrt',\n",
        "#                               min_samples_split=5,\n",
        "#                               max_depth=20,\n",
        "#                               min_samples_leaf=1)\n",
        "#model = GradientBoostingClassifier(n_estimators=50,\n",
        "#                               learning_rate=0.1,\n",
        "#                               max_depth=3)\n",
        "#model = LogisticRegression(C=100,\n",
        "#                           solver='liblinear')\n",
        "model = KNeighborsClassifier(n_neighbors=21,\n",
        "                           metric='manhattan')\n",
        "model.fit(rescaledX, y_train)\n",
        "\n",
        "# Estimativa da acur√°cia no conjunto de teste\n",
        "rescaledTestX = scaler.transform(X_test) # aplica√ß√£o da normaliza√ß√£o no conjunto de teste\n",
        "predictions = model.predict(rescaledTestX)\n",
        "print(accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvbZTO9rW_So"
      },
      "source": [
        "### Rodando o modelo a partir de um pipeline com os hiperpar√¢metros otimizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELHmSrgcXAVV",
        "outputId": "6368f37c-3d50-49d4-d401-bb2d761023ac"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)\n",
        "\n",
        "#model = RandomForestClassifier(n_estimators=50,\n",
        "#                               max_features='sqrt',\n",
        "#                               min_samples_split=2,\n",
        "#                               max_depth=10,\n",
        "#                               min_samples_leaf=1)\n",
        "#model = GradientBoostingClassifier(n_estimators=50,\n",
        "#                               learning_rate=0.1,\n",
        "#                               max_depth=3)\n",
        "#model = LogisticRegression(C=100,\n",
        "#                           solver='liblinear')\n",
        "model = KNeighborsClassifier(n_neighbors=21,\n",
        "                           metric='manhattan')\n",
        "\n",
        "pipeline = Pipeline(steps=[('MinMaxScaler', MinMaxScaler()), ('RF', model)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRCjy-R7XHgD"
      },
      "source": [
        "### Salvando os arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfO3NxckXIrT"
      },
      "outputs": [],
      "source": [
        "# Define o path\n",
        "directory = 'models'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o modelo\n",
        "#model_filename = 'rf_flights_classifier.pkl'\n",
        "#model_filename = 'gb_flights_classifier.pkl'\n",
        "#model_filename = 'flights_lr.pkl'\n",
        "model_filename = 'flights_knn.pkl'\n",
        "with open(os.path.join(directory, model_filename), 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Define o path\n",
        "directory = 'scalers'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o scaler\n",
        "scaler_filename = 'minmax_scaler_flights.pkl'\n",
        "with open(os.path.join(directory, scaler_filename), 'wb') as file:\n",
        "    pickle.dump(scaler, file)\n",
        "\n",
        "# Define o path\n",
        "directory = 'pipelines'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o pipeline\n",
        "pipeline_filename = 'rf_flights_pipeline.pkl'\n",
        "with open(os.path.join(directory, pipeline_filename), 'wb') as file:\n",
        "    pickle.dump(pipeline, file)\n",
        "\n",
        "# Salvando X_test e y_test\n",
        "column_names = ['DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_DELAY', 'SCHEDULED_ARRIVAL']\n",
        "X_test_df = pd.DataFrame(X_test, columns=column_names)\n",
        "column_names = ['DELAY_DETECTED']\n",
        "y_test_df = pd.DataFrame(y_test, columns=column_names)\n",
        "test_dataset_flights_df = pd.concat([X_test_df, y_test_df], axis=1)\n",
        "\n",
        "# Define o path\n",
        "directory = 'data'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "X_test_df.to_csv(os.path.join(directory, \"X_test_dataset_flights.csv\"), index=False)\n",
        "y_test_df.to_csv(os.path.join(directory, \"y_test_dataset_flights.csv\"), index=False)\n",
        "test_dataset_flights_df.to_csv(os.path.join(directory, \"test_dataset_flights.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-WeYEudXkTE"
      },
      "source": [
        "## Simulando a aplica√ß√£o do modelo em dados n√£o vistos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "U_FZ6nOFXnkj",
        "outputId": "9cfa2313-be55-4bee-ed6a-89c72bf5daeb"
      },
      "outputs": [],
      "source": [
        "# Prepara√ß√£o do modelo com TODO o dataset\n",
        "scaler = MinMaxScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
        "rescaledX = scaler.transform(X) # aplica√ß√£o da normaliza√ß√£o com TODO o dataset\n",
        "model.fit(rescaledX, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ariuJOryXq8R",
        "outputId": "f2db76e6-b271-4a7a-9226-fe237a1f7920"
      },
      "outputs": [],
      "source": [
        "# Novos dados - n√£o sabemos a classe!\n",
        "data = {'day':  [1, 12, 18],\n",
        "        'week': [1, 4, 6],\n",
        "        'airline': [3, 5, 10],\n",
        "        'flight_no': [100, 500, 800],\n",
        "        'tail': [500, 1500, 2000],\n",
        "        'origin': [50, 100, 200],\n",
        "        'destination': [80, 120, 210],\n",
        "        'dep_delay': [5, -8, 12],\n",
        "        'schedule_arrival': [415, 1309, 1845],\n",
        "        }\n",
        "\n",
        "atributos = ['day', 'week', 'airline', 'flight_no', 'tail', 'origin', 'destination', 'dep_delay', 'schedule_arrival']\n",
        "entrada = pd.DataFrame(data, columns=atributos)\n",
        "\n",
        "array_entrada = entrada.values\n",
        "X_entrada = array_entrada[:,0:9].astype(float)\n",
        "\n",
        "# Padroniza√ß√£o nos dados de entrada usando o scaler utilizado em X\n",
        "rescaledEntradaX = scaler.transform(X_entrada)\n",
        "print(rescaledEntradaX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCGTZn6Xscg",
        "outputId": "48366c76-12d0-4f29-d04b-19752b431c44"
      },
      "outputs": [],
      "source": [
        "# Predi√ß√£o de classes dos dados de entrada\n",
        "saidas = model.predict(rescaledEntradaX)\n",
        "print(saidas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMhvMoVxkPg"
      },
      "source": [
        "#Max e Min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-kKX80389Jp",
        "outputId": "200c7375-21f2-45e7-862e-2435f179e80a"
      },
      "outputs": [],
      "source": [
        "print(\"Max\")\n",
        "print(test_dataset_flights_df.max())\n",
        "print(\"Min\")\n",
        "print(test_dataset_flights_df.min())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOLpijF2mseZbsbpgRStKld",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1i6I8JD5u4jAhIMrC3aCnALjeRKshvgz1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
