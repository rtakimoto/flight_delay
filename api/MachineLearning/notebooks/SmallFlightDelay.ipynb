{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtakimoto/flight_delay/blob/main/tree/main/api/MachineLearning/notebooks/SmallFlightDelay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lejEmLi-9ukZ"
      },
      "source": [
        "#Geracao de Modelo para analise de atraso de voo\n",
        "Instalando as bibliotecas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8jcTI75BELH"
      },
      "source": [
        "Bibliotecas necessárias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8KDsISE9tgX"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# configuração para não exibir os warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports necessários\n",
        "import os\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tJ5u2t0-FXZ"
      },
      "source": [
        "#Carregamento do dataset (carga de um dataset pequeno para o MVP da PUC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1Pn1RTi_oUq",
        "outputId": "82674b97-41a3-42a7-ec6a-66a4ac7d7eab"
      },
      "outputs": [],
      "source": [
        "# Informa a URL de importação do dataset\n",
        "DatasetPath = kagglehub.dataset_download(\"eminhashimi/flight-delay-dataset\")\n",
        "print(\"Path to dataset files:\", DatasetPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "254M-cOsHKiQ",
        "outputId": "866a990b-da00-4ddd-bc89-68c5c1c515e6"
      },
      "outputs": [],
      "source": [
        "FlightDataset = pd.read_csv(DatasetPath+\"/flights_test.csv\", delimiter=',')\n",
        "\n",
        "# displaying the contents of the Test XLSX file\n",
        "FlightDataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWnWNMp3ISFD"
      },
      "source": [
        "##Pre Processamento do dataset carregado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvcSNodBIZXE",
        "outputId": "28485ad0-fc2c-45fc-9838-c4175f907f71"
      },
      "outputs": [],
      "source": [
        "FlightDataset.info()\n",
        "\n",
        "# Remove rows with null values\n",
        "FlightDatasetCleaned = FlightDataset.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBDVTZ1Rrphp",
        "outputId": "d45a76a5-5f11-499b-97d6-508a006a85b1"
      },
      "outputs": [],
      "source": [
        "print('Shape of the resulting Flight Dataset table: ', FlightDataset.shape)\n",
        "print('Shape of the resulting Flight Dataset table: ', FlightDatasetCleaned.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "kvac52uJxY_J",
        "outputId": "eacae939-4fb5-4aba-c5cc-c85c31c932a7"
      },
      "outputs": [],
      "source": [
        "FlightDatasetCleaned['DELAY_DETECTED'] = 0\n",
        "FlightDatasetCleaned.loc[(FlightDatasetCleaned['ARRIVAL_TIME']-FlightDatasetCleaned['SCHEDULED_ARRIVAL']) >0, 'DELAY_DETECTED'] = 1\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po8-isvXRe7l"
      },
      "source": [
        "##Diminuição do número de registros para o MVP da PUC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "y1fWcTM3SZ8n",
        "outputId": "ae60a056-9a8f-47e8-9691-07b36628f9a2"
      },
      "outputs": [],
      "source": [
        "FlightDatasetCleaned = FlightDatasetCleaned.sample(frac=0.02)\n",
        "print('Shape of the resulting Flight Dataset table: ', FlightDatasetCleaned.shape)\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "AhgE12ohxdUK",
        "outputId": "4090d976-5749-4c05-a257-f22cd0ec9990"
      },
      "outputs": [],
      "source": [
        "airline_names = FlightDatasetCleaned['AIRLINE'].unique()\n",
        "tail_names = FlightDatasetCleaned['TAIL_NUMBER'].unique()\n",
        "origin_names = FlightDatasetCleaned['ORIGIN_AIRPORT'].unique()\n",
        "destination_names = FlightDatasetCleaned['DESTINATION_AIRPORT'].unique()\n",
        "\n",
        "str_cols = FlightDatasetCleaned.columns[FlightDatasetCleaned.columns.str.contains('(?:AIRLINE|TAIL_NUMBER|ORIGIN_AIRPORT|DESTINATION_AIRPORT)')]\n",
        "clfs = {c:LabelEncoder() for c in str_cols}\n",
        "\n",
        "for col, clf in clfs.items():\n",
        "  FlightDatasetCleaned[col] = clfs[col].fit_transform(FlightDatasetCleaned[col])\n",
        "\n",
        "airline_values = FlightDatasetCleaned['AIRLINE'].unique()\n",
        "tail_values = FlightDatasetCleaned['TAIL_NUMBER'].unique()\n",
        "origin_values = FlightDatasetCleaned['ORIGIN_AIRPORT'].unique()\n",
        "destination_values = FlightDatasetCleaned['DESTINATION_AIRPORT'].unique()\n",
        "\n",
        "FlightDatasetCleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXPThYRfzwJu",
        "outputId": "d0ecef23-dc10-4332-c11e-cfa5e6a42265"
      },
      "outputs": [],
      "source": [
        "map_airline = np.array([airline_names, airline_values])\n",
        "column_names = ['AIRLINE', 'VALUES']\n",
        "airline_dataframe = pd.DataFrame(map_airline.transpose(), columns=column_names)\n",
        "print(airline_dataframe)\n",
        "\n",
        "map_tail = np.array([tail_names, tail_values])\n",
        "column_names = ['TAIL_NUMBER', 'VALUES']\n",
        "tail_dataframe = pd.DataFrame(map_tail.transpose(), columns=column_names)\n",
        "print(tail_dataframe)\n",
        "\n",
        "map_origin = np.array([origin_names, origin_values])\n",
        "column_names = ['ORIGIN_AIRPORT', 'VALUES']\n",
        "origin_dataframe = pd.DataFrame(map_origin.transpose(), columns=column_names)\n",
        "print(origin_dataframe)\n",
        "\n",
        "map_destination = np.array([destination_names, destination_values])\n",
        "column_names = ['DESTINATION_AIRPORT', 'VALUES']\n",
        "destination_dataframe = pd.DataFrame(map_destination.transpose(), columns=column_names)\n",
        "print(destination_dataframe)\n",
        "\n",
        "# Define o path\n",
        "directory = 'data'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "airline_dataframe.to_csv(os.path.join(directory, 'airline.csv'), index=False)\n",
        "tail_dataframe.to_csv(os.path.join(directory, 'tail.csv'), index=False)\n",
        "origin_dataframe.to_csv(os.path.join(directory, 'origin.csv'), index=False)\n",
        "destination_dataframe.to_csv(os.path.join(directory, 'destination.csv'), index=False)\n",
        "\n",
        "\n",
        "#print(clfs['AIRLINE'].inverse_transform(FlightDatasetCleaned['AIRLINE']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftqtjrGSroA-"
      },
      "source": [
        "# Separação em conjunto de treino e conjunto de teste com holdout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgyAlfODsCgx"
      },
      "outputs": [],
      "source": [
        "test_size = 0.20 # tamanho do conjunto de teste\n",
        "seed = 7 # semente aleatória\n",
        "\n",
        "# Separação em conjuntos de treino e teste\n",
        "array = FlightDatasetCleaned.values\n",
        "X = FlightDatasetCleaned[['DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_DELAY', 'SCHEDULED_ARRIVAL']].values\n",
        "y = FlightDatasetCleaned['DELAY_DETECTED'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, random_state=seed, stratify=y) # holdout com estratificação\n",
        "\n",
        "# Parâmetros e partições da validação cruzada\n",
        "scoring = 'accuracy'\n",
        "num_particoes = 10\n",
        "kfold = StratifiedKFold(n_splits=num_particoes, shuffle=True, random_state=seed) # validação cruzada com estratificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEHHNyDuNKN"
      },
      "source": [
        "#Modelagem e Inferência\n",
        "Criação e avaliação de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6cNGMPDIuScu",
        "outputId": "dc782e3d-22ec-4b2f-ac4b-500c37408336"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Lista que armazenará os modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os parâmetros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(bases)))\n",
        "\n",
        "# Listas para armazenar os resultados\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "# Avaliação dos modelos (treinamento)\n",
        "for name, model in models:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "fig.suptitle('Comparação dos Modelos')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyTQwB_6U95g"
      },
      "source": [
        "# Criação e avaliação de modelos: dados padronizados e normalizados\n",
        "\n",
        "Trabalhando com dados pontencialmente desbalanceados ou sensíveis a escala\n",
        "\n",
        "StandardScaler (padronização do conjunto de dados) e MinMaxScaler (normalização do conjunto de dados) são duas técnicas de normalização/escala usadas em machine learning para pré-processamento de dados e são úteis para preparar dados para algoritmos de aprendizado de máquina que são sensíveis à escala dos dados.\n",
        "\n",
        "## StandardScaler\n",
        "StandardScaler padroniza os dados, ou seja, remove a média e escala os dados para que tenham uma variância unitária. Ele transforma os dados para que a média de cada feature seja 0 e a variância seja 1.\n",
        "\n",
        "Fórmula: $z_i=\\frac{x_i-\\mu}{\\sigma}$\n",
        "\n",
        "\n",
        "onde:\n",
        "- $x_i$ é o valor original do $i$-ésimo termo da feature.\n",
        "- $\\mu$ é a média dos valores da feature.\n",
        "- $\\sigma$ é o desvio padrão dos valores da feature.\n",
        "𝑥\n",
        "x é o valor original da feature.\n",
        "𝜇\n",
        "μ é a média dos valores da feature.\n",
        "𝜎\n",
        "σ é o desvio padrão dos valores da feature.\n",
        "\n",
        "\n",
        "## MinMaxScaler\n",
        "MinMaxScaler escala e transforma os dados para um intervalo específico, geralmente entre 0 e 1. Ele transforma os dados para que o menor valor de uma feature seja 0 e o maior valor seja 1.\n",
        "\n",
        "Fórmula: $z_i=\\frac{x_i-min(x)}{max(x)-min(x)}$\n",
        "\n",
        "onde:\n",
        "- $x_i$ é o valor original do $i$-ésimo termo da feature.\n",
        "- $min(x)$ é o menor valor da feature.\n",
        "- $max(x)$ é o maior valor da feature.\n",
        "\n",
        "Nós vamos aplicar essas técnicas para os dados do dataset de diabetes através da construção de pipelines. Pipelines são uma maneira de simplificar o processo de construção de modelos, permitindo que você execute várias etapas de pré-processamento e modelagem em sequência."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uaRFYSSIU_9D",
        "outputId": "db2bb473-827e-412f-b506-6b62cf1c4d4c"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "# Listas para armazenar os armazenar os pipelines e os resultados para todas as visões do dataset\n",
        "pipelines = []\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "\n",
        "# Criando os elementos do pipeline\n",
        "\n",
        "# Algoritmos que serão utilizados\n",
        "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "cart = ('CART', DecisionTreeClassifier())\n",
        "naive_bayes = ('NB', GaussianNB())\n",
        "svm = ('SVM', SVC())\n",
        "bagging = ('Bag', BaggingClassifier(estimator=base, n_estimators=num_trees))\n",
        "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
        "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
        "voting = ('Voting', VotingClassifier(bases))\n",
        "\n",
        "# Transformações que serão utilizadas\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "\n",
        "# Montando os pipelines\n",
        "# A ordem de execução é da esquerda para a direita.\n",
        "\n",
        "# Dataset original\n",
        "pipelines.append(('LR-orig', Pipeline([reg_log])))\n",
        "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
        "pipelines.append(('CART-orig', Pipeline([cart])))\n",
        "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
        "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
        "pipelines.append(('Bag-orig', Pipeline([bagging])))\n",
        "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
        "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
        "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
        "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
        "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
        "\n",
        "# Dataset Padronizado\n",
        "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log])))\n",
        "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
        "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
        "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
        "pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging])))\n",
        "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
        "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
        "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
        "\n",
        "# Dataset Normalizado\n",
        "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log])))\n",
        "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
        "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
        "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
        "pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging])))\n",
        "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
        "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
        "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
        "\n",
        "# Executando os pipelines\n",
        "for name, model in pipelines:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
        "    print(msg)\n",
        "\n",
        "# Boxplot de comparação dos modelos\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "fig.suptitle('Comparação dos Modelos - Dataset original, padronizado e normalizado')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names, rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usd_XLdAVjvv"
      },
      "source": [
        "# Otimização dos hiperparâmetros\n",
        "\n",
        "A otimização de hiperparâmetros é o processo de encontrar os valores ideais para os hiperparâmetros de um modelo de machine learning. O objetivo é encontrar a combinação de hiperparâmetros que resulta no melhor desempenho do modelo.\n",
        "\n",
        "\n",
        "## Grid Search (*força bruta*)\n",
        "\n",
        "Como Funciona o Grid Search?\n",
        "1. Definição do Espaço de Hiperparâmetros: Primeiro, define-se um conjunto de valores possíveis para cada hiperparâmetro.\n",
        "2. Avaliação das Combinações: Em seguida, cada combinação possível desses valores é avaliada.\n",
        "3. Seleção do Melhor Conjunto: A combinação de hiperparâmetros que produz o melhor desempenho é selecionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FEZ2skqWcqw",
        "outputId": "12899f36-be89-46e6-e576-ae52d97bdcb7"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)  # Definindo uma semente global para este bloco\n",
        "\n",
        "# Lista de modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "\n",
        "# Definindo os parâmetros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# Criando os modelos para o VotingClassifier\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os ensembles e adicionando-os na lista de modelos\n",
        "models.append(('Bagging', BaggingClassifier(estimator=base, n_estimators=num_trees)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(estimators=bases, voting='hard')))\n",
        "\n",
        "# Definindo os componentes do pipeline\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "# Lista de pipelines\n",
        "pipelines = []\n",
        "\n",
        "# Criando pipelines para cada modelo\n",
        "for name, model in models:\n",
        "    pipelines.append((name + '-orig', Pipeline(steps=[(name, model)])))\n",
        "    pipelines.append((name + '-padr', Pipeline(steps=[standard_scaler, (name, model)])))\n",
        "    pipelines.append((name + '-norm', Pipeline(steps=[min_max_scaler, (name, model)])))\n",
        "\n",
        "# Definindo os parâmetros para GridSearchCV\n",
        "param_grids = {\n",
        "    'LR': {\n",
        "        'LR__C': [0.01, 0.1, 1, 10, 100],\n",
        "        'LR__solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'KNN': {\n",
        "        'KNN__n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
        "        'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
        "    },\n",
        "    'CART': {\n",
        "        'CART__max_depth': [None, 10, 20, 30, 40, 50],\n",
        "        'CART__min_samples_split': [2, 5, 10],\n",
        "        'CART__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'NB': {\n",
        "        'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
        "    },\n",
        "    'SVM': {\n",
        "        #'SVM__C': [0.1, 1, 10, 100],\n",
        "        'SVM__C': [1, 10],\n",
        "        #'SVM__gamma': [1, 0.1, 0.01, 0.001],\n",
        "        'SVM__gamma': [1, 0.1],\n",
        "        'SVM__kernel': ['rbf', 'linear']\n",
        "    },\n",
        "    'RF': {\n",
        "        'RF__n_estimators': [10, 50, 100, 200],\n",
        "        'RF__max_features': ['auto', 'sqrt', 'log2'],\n",
        "        'RF__max_depth': [None, 10, 20, 30],\n",
        "        'RF__min_samples_split': [2, 5, 10],\n",
        "        'RF__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'ET': {\n",
        "        'ET__n_estimators': [10, 50, 100, 200],\n",
        "        'ET__max_features': ['auto', 'sqrt', 'log2'],\n",
        "        'ET__max_depth': [None, 10, 20, 30],\n",
        "        'ET__min_samples_split': [2, 5, 10],\n",
        "        'ET__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Ada': {\n",
        "        'Ada__n_estimators': [10, 50, 100, 200],\n",
        "        'Ada__learning_rate': [0.01, 0.1, 1, 10]\n",
        "    },\n",
        "    'GB': {\n",
        "        'GB__n_estimators': [10, 50, 100, 200],\n",
        "        'GB__learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "        'GB__max_depth': [3, 5, 7, 9]\n",
        "    },\n",
        "    'Voting': {\n",
        "        # Para VotingClassifier, geralmente não há hiperparâmetros para ajustar diretamente\n",
        "        # Ajustar os hiperparâmetros dos estimadores base individualmente se necessário\n",
        "    }\n",
        "}\n",
        "\n",
        "# Parâmetros de cross-validation e scoring\n",
        "scoring = 'accuracy'\n",
        "kfold = 5\n",
        "\n",
        "# Executando o GridSearchCV para cada pipeline\n",
        "for name, pipeline in pipelines:\n",
        "    model_type = name.split('-')[0]\n",
        "    if model_type in param_grids:\n",
        "        param_grid = param_grids[model_type]\n",
        "    else:\n",
        "        param_grid = {}  # Para modelos que não têm parâmetros definidos\n",
        "\n",
        "    grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "    # Imprimindo a melhor configuração\n",
        "    print(\"Modelo: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-usY_T1yWvMN",
        "outputId": "7dff6ca1-0a90-42ca-d716-1eaddd9d9af1"
      },
      "outputs": [],
      "source": [
        "# Tuning do KNN\n",
        "\n",
        "np.random.seed(7) # definindo uma semente global para este bloco\n",
        "\n",
        "pipelines = []\n",
        "\n",
        "# Definindo os componentes do pipeline\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "pipelines.append(('knn-orig', Pipeline(steps=[knn])))\n",
        "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
        "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
        "\n",
        "param_grid = {\n",
        "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
        "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
        "}\n",
        "\n",
        "# Prepara e executa o GridSearchCV\n",
        "for name, model in pipelines:\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcdhA7WnWzIo"
      },
      "source": [
        "# Finalização do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyI-RG0lW2PU",
        "outputId": "e7b75644-a644-46b5-e7a2-2224f9fb1d38"
      },
      "outputs": [],
      "source": [
        "# Avaliação do modelo com o conjunto de testes\n",
        "# Melhor modelo\n",
        "# Modelo: GB-norm - Melhor: 0.823716 usando {'GB__learning_rate': 0.1, 'GB__max_depth': 3, 'GB__n_estimators': 50}\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "# Preparação do modelo\n",
        "scaler = MinMaxScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train) # aplicação da normalização no conjunto de treino\n",
        "#model = RandomForestClassifier(n_estimators=100,\n",
        "#                               max_features='sqrt',\n",
        "#                               min_samples_split=5,\n",
        "#                               max_depth=20,\n",
        "#                               min_samples_leaf=1)\n",
        "#model = GradientBoostingClassifier(n_estimators=50,\n",
        "#                               learning_rate=0.1,\n",
        "#                               max_depth=3)\n",
        "#model = LogisticRegression(C=100,\n",
        "#                           solver='liblinear')\n",
        "model = KNeighborsClassifier(n_neighbors=21,\n",
        "                           metric='manhattan')\n",
        "model.fit(rescaledX, y_train)\n",
        "\n",
        "# Estimativa da acurácia no conjunto de teste\n",
        "rescaledTestX = scaler.transform(X_test) # aplicação da normalização no conjunto de teste\n",
        "predictions = model.predict(rescaledTestX)\n",
        "print(accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvbZTO9rW_So"
      },
      "source": [
        "### Rodando o modelo a partir de um pipeline com os hiperparâmetros otimizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELHmSrgcXAVV",
        "outputId": "6368f37c-3d50-49d4-d401-bb2d761023ac"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)\n",
        "\n",
        "#model = RandomForestClassifier(n_estimators=50,\n",
        "#                               max_features='sqrt',\n",
        "#                               min_samples_split=2,\n",
        "#                               max_depth=10,\n",
        "#                               min_samples_leaf=1)\n",
        "#model = GradientBoostingClassifier(n_estimators=50,\n",
        "#                               learning_rate=0.1,\n",
        "#                               max_depth=3)\n",
        "#model = LogisticRegression(C=100,\n",
        "#                           solver='liblinear')\n",
        "model = KNeighborsClassifier(n_neighbors=21,\n",
        "                           metric='manhattan')\n",
        "\n",
        "pipeline = Pipeline(steps=[('MinMaxScaler', MinMaxScaler()), ('RF', model)])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "predictions = pipeline.predict(X_test)\n",
        "print(accuracy_score(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRCjy-R7XHgD"
      },
      "source": [
        "### Salvando os arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfO3NxckXIrT"
      },
      "outputs": [],
      "source": [
        "# Define o path\n",
        "directory = 'models'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o modelo\n",
        "#model_filename = 'rf_flights_classifier.pkl'\n",
        "#model_filename = 'gb_flights_classifier.pkl'\n",
        "#model_filename = 'flights_lr.pkl'\n",
        "model_filename = 'flights_knn.pkl'\n",
        "with open(os.path.join(directory, model_filename), 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Define o path\n",
        "directory = 'scalers'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o scaler\n",
        "scaler_filename = 'minmax_scaler_flights.pkl'\n",
        "with open(os.path.join(directory, scaler_filename), 'wb') as file:\n",
        "    pickle.dump(scaler, file)\n",
        "\n",
        "# Define o path\n",
        "directory = 'pipelines'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "# Salvando o pipeline\n",
        "pipeline_filename = 'rf_flights_pipeline.pkl'\n",
        "with open(os.path.join(directory, pipeline_filename), 'wb') as file:\n",
        "    pickle.dump(pipeline, file)\n",
        "\n",
        "# Salvando X_test e y_test\n",
        "column_names = ['DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER', 'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_DELAY', 'SCHEDULED_ARRIVAL']\n",
        "X_test_df = pd.DataFrame(X_test, columns=column_names)\n",
        "column_names = ['DELAY_DETECTED']\n",
        "y_test_df = pd.DataFrame(y_test, columns=column_names)\n",
        "test_dataset_flights_df = pd.concat([X_test_df, y_test_df], axis=1)\n",
        "\n",
        "# Define o path\n",
        "directory = 'data'\n",
        "\n",
        "#Salvando os mapeamentos\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)\n",
        "\n",
        "X_test_df.to_csv(os.path.join(directory, \"X_test_dataset_flights.csv\"), index=False)\n",
        "y_test_df.to_csv(os.path.join(directory, \"y_test_dataset_flights.csv\"), index=False)\n",
        "test_dataset_flights_df.to_csv(os.path.join(directory, \"test_dataset_flights.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-WeYEudXkTE"
      },
      "source": [
        "## Simulando a aplicação do modelo em dados não vistos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "U_FZ6nOFXnkj",
        "outputId": "9cfa2313-be55-4bee-ed6a-89c72bf5daeb"
      },
      "outputs": [],
      "source": [
        "# Preparação do modelo com TODO o dataset\n",
        "scaler = MinMaxScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
        "rescaledX = scaler.transform(X) # aplicação da normalização com TODO o dataset\n",
        "model.fit(rescaledX, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ariuJOryXq8R",
        "outputId": "f2db76e6-b271-4a7a-9226-fe237a1f7920"
      },
      "outputs": [],
      "source": [
        "# Novos dados - não sabemos a classe!\n",
        "data = {'day':  [1, 12, 18],\n",
        "        'week': [1, 4, 6],\n",
        "        'airline': [3, 5, 10],\n",
        "        'flight_no': [100, 500, 800],\n",
        "        'tail': [500, 1500, 2000],\n",
        "        'origin': [50, 100, 200],\n",
        "        'destination': [80, 120, 210],\n",
        "        'dep_delay': [5, -8, 12],\n",
        "        'schedule_arrival': [415, 1309, 1845],\n",
        "        }\n",
        "\n",
        "atributos = ['day', 'week', 'airline', 'flight_no', 'tail', 'origin', 'destination', 'dep_delay', 'schedule_arrival']\n",
        "entrada = pd.DataFrame(data, columns=atributos)\n",
        "\n",
        "array_entrada = entrada.values\n",
        "X_entrada = array_entrada[:,0:9].astype(float)\n",
        "\n",
        "# Padronização nos dados de entrada usando o scaler utilizado em X\n",
        "rescaledEntradaX = scaler.transform(X_entrada)\n",
        "print(rescaledEntradaX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmCGTZn6Xscg",
        "outputId": "48366c76-12d0-4f29-d04b-19752b431c44"
      },
      "outputs": [],
      "source": [
        "# Predição de classes dos dados de entrada\n",
        "saidas = model.predict(rescaledEntradaX)\n",
        "print(saidas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMhvMoVxkPg"
      },
      "source": [
        "#Max e Min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-kKX80389Jp",
        "outputId": "200c7375-21f2-45e7-862e-2435f179e80a"
      },
      "outputs": [],
      "source": [
        "print(\"Max\")\n",
        "print(test_dataset_flights_df.max())\n",
        "print(\"Min\")\n",
        "print(test_dataset_flights_df.min())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOLpijF2mseZbsbpgRStKld",
      "gpuType": "T4",
      "include_colab_link": true,
      "mount_file_id": "1i6I8JD5u4jAhIMrC3aCnALjeRKshvgz1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
